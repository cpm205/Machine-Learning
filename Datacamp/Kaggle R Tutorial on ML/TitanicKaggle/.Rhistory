test_one$Survived[test_one$Sex == "female"] <- 1
# Load in the R package
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
# Build the decision tree
my_tree_two <- rpart(Survived  ~ Pclass + Sex + Age + SibSp + Parch + Fare  + Embarked, data = train, method = "class")
# Visualize the decision tree using plot() and text()
plot(my_tree_two)
text(my_tree_two)
# Time to plot your fancy tree
fancyRpartPlot(my_tree_two)
# Make predictions on the test set
my_prediction <- predict(my_tree_two, newdata = test, type = "class")
# Finish the data.frame() call
my_solution <- data.frame(PassengerId = test$PassengerId, Survived = my_prediction)
# Use nrow() on my_solution
nrow(my_solution)
# Finish the write.csv() call
write.csv(my_solution, file = "my_solution.csv", row.names = FALSE)
# Change this command
my_tree_three  <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
data = train, method = "class", control = rpart.control(minsplit = 50, cp = 0))
# Visualize my_tree_three
fancyRpartPlot(my_tree_three)
#Using feature Engineering to Re-engineering our Titanic data set
#A valid assumption is that larger families need more time to get together on a sinking ship,
#and hence have less chance of surviving. Family size is determined by the variables SibSp and Parch,
#which indicate the number of family members a certain passenger is traveling with.
#So when doing feature engineering, you add a new variable family_size,
#which is the sum of SibSp and Parch plus one (the observation itself), to the test and train set.
# Create train_two
train_two <- train
train_two$family_size <- train_two$SibSp + train_two$Parch + 1
# Finish the command
my_tree_four <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + family_size,
data = train_two, method = "class")
# Visualize your new decision tree
fancyRpartPlot(my_tree_four)
View(train_two)
View(train_two)
View(test_one)
#Add title column to train_two
train_two$Title <- NA
grep("Mr",train_two$Name,value = TRUE)
index(train_two$Name,"Mr")
grep(train_two$Name,"Mr")
grep(train_two[0]$Name,"Mr")
grep(train_two[0].Name,"Mr")
grep("mr sdfsdf","Mr")
grep("sdfsdf","Mr")
grep("sdfsdf mr","Mr")
grep("sdfsdf Mr","Mr")
regexpr("sdfsdf Mr","Mr")
install.packages("stringr")
str_locate("aabcd", "bcd")
str_locate("aabcd", "bcd")
str_locate("aabcd", "bcd")
# Import the training set: train
train_url <- "http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv"
train <- read.csv(train_url)
# Import the testing set: test
test_url <- "http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv"
test <- read.csv(test_url)
# Print train and test to the console
print(train)
print(test)
# Your train and test set are still loaded
str(train)
str(test)
# Your train and test set are still loaded
str(train)
str(test)
# Survival rates in absolute numbers
table(train$Survived)
# Survival rates in proportions
prop.table(table(train$Survived))
# Two-way comparison: Sex and Survived
table(train$Sex, train$Survived)
# Two-way comparison: row-wise proportions
prop.table(table(train$Sex, train$Survived),1)
# Create the column child, and indicate whether child or no child
train$Child <- NA
train$Child[train$Age<18] <- 1
train$Child[train$Age>= 18] <- 0
# Two-way comparison
prop.table(table(train$Child, train$Survived),1)
# Copy of test
test_one <- test
# Initialize a Survived column to 0
test_one$Survived <- 0
# Set Survived to 1 if Sex equals "female"
test_one$Survived[test_one$Sex == "female"] <- 1
# Load in the R package
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(stringr)
# Build the decision tree
my_tree_two <- rpart(Survived  ~ Pclass + Sex + Age + SibSp + Parch + Fare  + Embarked, data = train, method = "class")
# Visualize the decision tree using plot() and text()
plot(my_tree_two)
text(my_tree_two)
# Time to plot your fancy tree
fancyRpartPlot(my_tree_two)
# Make predictions on the test set
my_prediction <- predict(my_tree_two, newdata = test, type = "class")
# Finish the data.frame() call
my_solution <- data.frame(PassengerId = test$PassengerId, Survived = my_prediction)
# Use nrow() on my_solution
nrow(my_solution)
# Finish the write.csv() call
write.csv(my_solution, file = "my_solution.csv", row.names = FALSE)
# Change this command
my_tree_three  <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
data = train, method = "class", control = rpart.control(minsplit = 50, cp = 0))
# Visualize my_tree_three
fancyRpartPlot(my_tree_three)
#Using feature Engineering to Re-engineering our Titanic data set
#A valid assumption is that larger families need more time to get together on a sinking ship,
#and hence have less chance of surviving. Family size is determined by the variables SibSp and Parch,
#which indicate the number of family members a certain passenger is traveling with.
#So when doing feature engineering, you add a new variable family_size,
#which is the sum of SibSp and Parch plus one (the observation itself), to the test and train set.
# Create train_two
train_two <- train
train_two$family_size <- train_two$SibSp + train_two$Parch + 1
# Finish the command
my_tree_four <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + family_size,
data = train_two, method = "class")
# Visualize your new decision tree
fancyRpartPlot(my_tree_four)
#Add title column to train_two
str_locate("aabcd", "bcd")
str_locate("aabcd", "n")
str_locate("aabcd", "n") == NA
if(str_locate("aabcd", "n") == NA) print"yes"
if(str_locate("aabcd", "n") == NA) print "yes"
If (str_locate("aabcd", "n") == NA) print "yes"
grepl("Mr", "MR")
grepl("Mr", "Mr")
grepl(train_two$Name, "Mr")
tain_two[0]
train_two[0]
train_two[0,1]
train_two[1:1,]
train_two[1:1,]$Name
grepl(train_two[1:1,]$Name, "Mr")
grepl(train_two[1:1,]$Name, "Mr.")
grepl(train_two[1:1,]$Name, "Mr")
grepl("Mr", "Mr")
train_two[1:1,]$Name
grepl(train_two[1:1,]$Name, "Mr")
train_two[1:1,]
train_two[1:1,4]
grepl(train_two[1:1,4], "Mr")
train_two[1:1,4]
train_two[1:1,4]
toString(train_two[1:1,4])
grepl(toString(train_two[1:1,4]), "Mr")
toString(train_two[1:1,4])
grepl(toString(train_two[1:1,4]), "Mr.")
grepl("Mr", "Mr.")
View(test_one)
View(train_two)
View(train)
View(train_two)
View(my_solution)
View(my_solution)
View(test_one)
load("C:/Users/derekh/Downloads/all_data.RData")
View(all_data)
View(all_data)
all_data$Fare[1044] <- median(all_data$Fare, na.rm = TRUE)
# Passenger on row 62 and 830 do not have a value for embarkment.
# Since many passengers embarked at Southampton, we give them the value S.
all_data$Embarked[c(62, 830)] <- "S"
# Factorize embarkment codes.
all_data$Embarked <- factor(all_data$Embarked)
# Passenger on row 1044 has an NA Fare value. Let's replace it with the median fare value.
all_data$Fare[1044] <- median(all_data$Fare, na.rm = TRUE)
# How to fill in missing Age values?
# We make a prediction of a passengers Age using the other variables and a decision tree model.
# This time you give method = "anova" since you are predicting a continuous variable.
library(rpart)
predicted_age <- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + Title + family_size,
data = all_data[!is.na(all_data$Age),], method = "anova")
all_data$Age[is.na(all_data$Age)] <- predict(predicted_age, all_data[is.na(all_data$Age),])
# Split the data back into a train set and a test set
train <- all_data[1:891,]
test <- all_data[892:1309,]
# Import the training set: train
train_url <- "http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv"
train <- read.csv(train_url)
# Import the testing set: test
test_url <- "http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv"
test <- read.csv(test_url)
# Print train and test to the console
print(train)
print(test)
# Your train and test set are still loaded
str(train)
str(test)
# Survival rates in absolute numbers
table(train$Survived)
# Survival rates in proportions
prop.table(table(train$Survived))
# Two-way comparison: Sex and Survived
table(train$Sex, train$Survived)
# Two-way comparison: row-wise proportions
prop.table(table(train$Sex, train$Survived),1)
# Create the column child, and indicate whether child or no child
train$Child <- NA
train$Child[train$Age<18] <- 1
train$Child[train$Age>= 18] <- 0
# Two-way comparison
prop.table(table(train$Child, train$Survived),1)
# Copy of test
test_one <- test
# Initialize a Survived column to 0
test_one$Survived <- 0
# Set Survived to 1 if Sex equals "female"
test_one$Survived[test_one$Sex == "female"] <- 1
# Load in the R package
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(stringr)
# Build the decision tree
my_tree_two <- rpart(Survived  ~ Pclass + Sex + Age + SibSp + Parch + Fare  + Embarked, data = train, method = "class")
# Visualize the decision tree using plot() and text()
plot(my_tree_two)
text(my_tree_two)
# Time to plot your fancy tree
fancyRpartPlot(my_tree_two)
# Make predictions on the test set
my_prediction <- predict(my_tree_two, newdata = test, type = "class")
# Finish the data.frame() call
my_solution <- data.frame(PassengerId = test$PassengerId, Survived = my_prediction)
# Use nrow() on my_solution
nrow(my_solution)
# Finish the write.csv() call
write.csv(my_solution, file = "my_solution.csv", row.names = FALSE)
# Change this command
my_tree_three  <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
data = train, method = "class", control = rpart.control(minsplit = 50, cp = 0))
# Visualize my_tree_three
fancyRpartPlot(my_tree_three)
#Using feature Engineering to Re-engineering our Titanic data set
#A valid assumption is that larger families need more time to get together on a sinking ship,
#and hence have less chance of surviving. Family size is determined by the variables SibSp and Parch,
#which indicate the number of family members a certain passenger is traveling with.
#So when doing feature engineering, you add a new variable family_size,
#which is the sum of SibSp and Parch plus one (the observation itself), to the test and train set.
# Create train_two
train_two <- train
train_two$family_size <- train_two$SibSp + train_two$Parch + 1
# Finish the command
my_tree_four <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + family_size,
data = train_two, method = "class")
# Visualize your new decision tree
fancyRpartPlot(my_tree_four)
# Passenger on row 62 and 830 do not have a value for embarkment.
# Since many passengers embarked at Southampton, we give them the value S.
all_data$Embarked[c(62, 830)] <- "S"
# Factorize embarkment codes.
all_data$Embarked <- factor(all_data$Embarked)
# Passenger on row 1044 has an NA Fare value. Let's replace it with the median fare value.
all_data$Fare[1044] <- median(all_data$Fare, na.rm = TRUE)
# How to fill in missing Age values?
# We make a prediction of a passengers Age using the other variables and a decision tree model.
# This time you give method = "anova" since you are predicting a continuous variable.
predicted_age <- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + Title + family_size,
data = all_data[!is.na(all_data$Age),], method = "anova")
all_data$Age[is.na(all_data$Age)] <- predict(predicted_age, all_data[is.na(all_data$Age),])
# Split the data back into a train set and a test set
train <- all_data[1:891,]
test <- all_data[892:1309,]
# Load in the package
library(randomForest)
# Train set and test set
str(train)
str(test)
# Set seed for reproducibility
set.seed(111)
# Apply the Random Forest Algorithm
my_forest <- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare +
Embarked + Title,  data=train, importance=TRUE, ntree=1000)
# Make your prediction using the test set
my_prediction <- predict(my_forest, test)
# Create a data frame with two columns: PassengerId & Survived. Survived contains your predictions
my_solution <- data.frame(PassengerId = test$PassengerId, Survived = my_prediction)
# Write your solution away to a csv file with the name my_solution.csv
write.csv(my_solution, file = "my_solution2.csv", row.names = FALSE)
# Import the training set: train
train_url <- "http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv"
train <- read.csv(train_url)
# Import the testing set: test
test_url <- "http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv"
test <- read.csv(test_url)
# Print train and test to the console
print(train)
print(test)
# Your train and test set are still loaded
str(train)
str(test)
# Survival rates in absolute numbers
table(train$Survived)
# Survival rates in proportions
prop.table(table(train$Survived))
# Two-way comparison: Sex and Survived
table(train$Sex, train$Survived)
# Two-way comparison: row-wise proportions
prop.table(table(train$Sex, train$Survived),1)
# Create the column child, and indicate whether child or no child
train$Child <- NA
train$Child[train$Age<18] <- 1
train$Child[train$Age>= 18] <- 0
# Two-way comparison
prop.table(table(train$Child, train$Survived),1)
# Copy of test
test_one <- test
# Initialize a Survived column to 0
test_one$Survived <- 0
# Set Survived to 1 if Sex equals "female"
test_one$Survived[test_one$Sex == "female"] <- 1
# Load in the R package
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(stringr)
# Build the decision tree
my_tree_two <- rpart(Survived  ~ Pclass + Sex + Age + SibSp + Parch + Fare  + Embarked, data = train, method = "class")
# Visualize the decision tree using plot() and text()
plot(my_tree_two)
text(my_tree_two)
# Time to plot your fancy tree
fancyRpartPlot(my_tree_two)
# Make predictions on the test set
my_prediction <- predict(my_tree_two, newdata = test, type = "class")
# Finish the data.frame() call
my_solution <- data.frame(PassengerId = test$PassengerId, Survived = my_prediction)
# Use nrow() on my_solution
nrow(my_solution)
# Finish the write.csv() call
write.csv(my_solution, file = "my_solution.csv", row.names = FALSE)
# Change this command
my_tree_three  <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
data = train, method = "class", control = rpart.control(minsplit = 50, cp = 0))
# Visualize my_tree_three
fancyRpartPlot(my_tree_three)
#Using feature Engineering to Re-engineering our Titanic data set
#A valid assumption is that larger families need more time to get together on a sinking ship,
#and hence have less chance of surviving. Family size is determined by the variables SibSp and Parch,
#which indicate the number of family members a certain passenger is traveling with.
#So when doing feature engineering, you add a new variable family_size,
#which is the sum of SibSp and Parch plus one (the observation itself), to the test and train set.
# Create train_two
train_two <- train
train_two$family_size <- train_two$SibSp + train_two$Parch + 1
# Finish the command
my_tree_four <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + family_size,
data = train_two, method = "class")
# Visualize your new decision tree
fancyRpartPlot(my_tree_four)
# Passenger on row 62 and 830 do not have a value for embarkment.
# Since many passengers embarked at Southampton, we give them the value S.
all_data$Embarked[c(62, 830)] <- "S"
# Factorize embarkment codes.
all_data$Embarked <- factor(all_data$Embarked)
# Passenger on row 1044 has an NA Fare value. Let's replace it with the median fare value.
all_data$Fare[1044] <- median(all_data$Fare, na.rm = TRUE)
# How to fill in missing Age values?
# We make a prediction of a passengers Age using the other variables and a decision tree model.
# This time you give method = "anova" since you are predicting a continuous variable.
predicted_age <- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + Title + family_size,
data = all_data[!is.na(all_data$Age),], method = "anova")
all_data$Age[is.na(all_data$Age)] <- predict(predicted_age, all_data[is.na(all_data$Age),])
# Split the data back into a train set and a test set
train <- all_data[1:891,]
test <- all_data[892:1309,]
# Load in the package
library(randomForest)
# Train set and test set
str(train)
str(test)
# Set seed for reproducibility
set.seed(111)
# Apply the Random Forest Algorithm
my_forest <- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare +
Embarked + Title,  data=train, importance=TRUE, ntree=1000)
# Make your prediction using the test set
my_prediction2 <- predict(my_forest, test)
# Create a data frame with two columns: PassengerId & Survived. Survived contains your predictions
my_solution2 <- data.frame(PassengerId = test$PassengerId, Survived = my_prediction2)
# Write your solution away to a csv file with the name my_solution.csv
write.csv(my_solution2, file = "my_solution2.csv", row.names = FALSE)
install.packages("randomForest")
# Import the training set: train
train_url <- "http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv"
train <- read.csv(train_url)
# Import the testing set: test
test_url <- "http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv"
test <- read.csv(test_url)
# Print train and test to the console
print(train)
print(test)
# Your train and test set are still loaded
str(train)
str(test)
# Survival rates in absolute numbers
table(train$Survived)
# Survival rates in proportions
prop.table(table(train$Survived))
# Two-way comparison: Sex and Survived
table(train$Sex, train$Survived)
# Two-way comparison: row-wise proportions
prop.table(table(train$Sex, train$Survived),1)
# Create the column child, and indicate whether child or no child
train$Child <- NA
train$Child[train$Age<18] <- 1
train$Child[train$Age>= 18] <- 0
# Two-way comparison
prop.table(table(train$Child, train$Survived),1)
# Copy of test
test_one <- test
# Initialize a Survived column to 0
test_one$Survived <- 0
# Set Survived to 1 if Sex equals "female"
test_one$Survived[test_one$Sex == "female"] <- 1
# Load in the R package
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(stringr)
# Build the decision tree
my_tree_two <- rpart(Survived  ~ Pclass + Sex + Age + SibSp + Parch + Fare  + Embarked, data = train, method = "class")
# Visualize the decision tree using plot() and text()
plot(my_tree_two)
text(my_tree_two)
# Time to plot your fancy tree
fancyRpartPlot(my_tree_two)
# Make predictions on the test set
my_prediction <- predict(my_tree_two, newdata = test, type = "class")
# Finish the data.frame() call
my_solution <- data.frame(PassengerId = test$PassengerId, Survived = my_prediction)
# Use nrow() on my_solution
nrow(my_solution)
# Finish the write.csv() call
write.csv(my_solution, file = "my_solution.csv", row.names = FALSE)
# Change this command
my_tree_three  <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
data = train, method = "class", control = rpart.control(minsplit = 50, cp = 0))
# Visualize my_tree_three
fancyRpartPlot(my_tree_three)
#Using feature Engineering to Re-engineering our Titanic data set
#A valid assumption is that larger families need more time to get together on a sinking ship,
#and hence have less chance of surviving. Family size is determined by the variables SibSp and Parch,
#which indicate the number of family members a certain passenger is traveling with.
#So when doing feature engineering, you add a new variable family_size,
#which is the sum of SibSp and Parch plus one (the observation itself), to the test and train set.
# Create train_two
train_two <- train
train_two$family_size <- train_two$SibSp + train_two$Parch + 1
# Finish the command
my_tree_four <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + family_size,
data = train_two, method = "class")
# Visualize your new decision tree
fancyRpartPlot(my_tree_four)
# Passenger on row 62 and 830 do not have a value for embarkment.
# Since many passengers embarked at Southampton, we give them the value S.
all_data$Embarked[c(62, 830)] <- "S"
# Factorize embarkment codes.
all_data$Embarked <- factor(all_data$Embarked)
# Passenger on row 1044 has an NA Fare value. Let's replace it with the median fare value.
all_data$Fare[1044] <- median(all_data$Fare, na.rm = TRUE)
# How to fill in missing Age values?
# We make a prediction of a passengers Age using the other variables and a decision tree model.
# This time you give method = "anova" since you are predicting a continuous variable.
predicted_age <- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + Title + family_size,
data = all_data[!is.na(all_data$Age),], method = "anova")
all_data$Age[is.na(all_data$Age)] <- predict(predicted_age, all_data[is.na(all_data$Age),])
# Split the data back into a train set and a test set
train <- all_data[1:891,]
test <- all_data[892:1309,]
# Load in the package
library(randomForest)
# Train set and test set
str(train)
str(test)
# Set seed for reproducibility
set.seed(111)
# Apply the Random Forest Algorithm
my_forest <- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare +
Embarked + Title,  data=train, importance=TRUE, ntree=1000)
# Make your prediction using the test set
my_prediction2 <- predict(my_forest, test)
# Create a data frame with two columns: PassengerId & Survived. Survived contains your predictions
my_solution2 <- data.frame(PassengerId = test$PassengerId, Survived = my_prediction2)
# Write your solution away to a csv file with the name my_solution.csv
write.csv(my_solution2, file = "my_solution2.csv", row.names = FALSE)
# Train set and test set
str(train)
str(test)
View(train)
View(train_two)
varImpPlot(my_forest)
