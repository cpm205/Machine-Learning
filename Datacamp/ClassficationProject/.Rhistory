library(readr)
train <- read_csv("C:/MyStuff/Machine Learning/Datacamp/ClassficationProject/train.csv")
View(train)
library(readr)
test <- read_csv("C:/MyStuff/Machine Learning/Datacamp/ClassficationProject/test.csv")
View(test)
print(train)
str(train)
view(train)
str(train)
library(rpart)
install.packages("rpart")
help
help.search(rpart)
print(tree)
tree <- rpart(Survived ~ ., train, method = "class")
train <- read.csv("train.csv", stringsAsFactors=FALSE)
train
table(train$Survived)
text(fit)
library(rpart)
source('C:/MyStuff/Machine Learning/Datacamp/ClassficationProject/DecisionTreeExample.R')
print(train)
plot(tree)
plot(tree)
plot(fit)
plot(fit)
plot(fit)
plot(fit)
plot(fit)
plot(fit)
plot(tree)
print(conf)
plot(tree)
print(conf)
library(rpart)
library(rpart)
print(train)
tree <- rpart(Survived ~ ., train, method = "class")
pred <- predict(tree,test,type = "class")
conf <- table(test$Survived, pred)
plot(tree)
print(conf)
library(rpart)
print(train)
tree <- rpart(Survived ~ ., train, method = "class")
pred <- predict(tree,test,type = "class")
conf <- table(test$Survived, pred)
plot(tree)
print(conf)
library(rpart)
print(train)
tree <- rpart(Survived ~ ., train, method = "class")
pred <- predict(tree,test,type = "class")
conf <- table(test$Survived, pred)
plot(tree)
print(conf)
library(rpart)
print(train)
tree <- rpart(Survived ~ ., train, method = "class")
pred <- predict(tree,test,type = "class")
conf <- table(test$Survived, pred)
plot(tree)
print(conf)
library(rpart)
print(train)
tree <- rpart(Survived ~ ., train, method = "class")
library(rpart)
print(train)
tree <- rpart(Survived ~ ., train, method = "class")
plot(tree)
library(rpart)
print(train)
tree <- rpart(Survived ~ ., train, method = "class")
plot(tree)
library(rpart)
print(train)
tree <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
data=train,
method="class")
plot(tree)
library(rpart)
print(train)
tree <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
data=train,
method="class")
plot(tree)
text(fit)
library(rpart)
str(train)
tree <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
data=train,
method="class")
plot(tree)
text(fit)
library(rpart)
rpart(Survived ~ ., train, method = "class")(train)
#tree <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data=train,method="class")
tree <- rpart(Survived ~ ., train, method = "class")
plot(tree)
text(fit)
library(rpart)
rpart(Survived ~ ., train, method = "class")(train)
#tree <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data=train,method="class")
tree <- rpart(Survived ~ ., train, method = "class")
plot(tree)
text(fit)
source('C:/MyStuff/Machine Learning/Datacamp/ClassficationProject/DecisionTreeExample.R')
source('C:/MyStuff/Machine Learning/Datacamp/ClassficationProject/DecisionTreeExample.R')
library(rpart)
rpart(Survived ~ ., train, method = "class")(train)
tree <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data=train,method="class")
#tree <- rpart(Survived ~ ., train, method = "class")
plot(tree)
text(fit)
library(rpart)
rpart(Survived ~ ., train, method = "class")(train)
tree <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data=train,method="class")
#tree <- rpart(Survived ~ ., train, method = "class")
pred <- predict(tree,test,type = "class")
# Calculate the confusion matrix: conf
conf <- table(test$Survived, pred)
# Print this confusion matrix
print(conf)
plot(tree)
text(fit)
library(rpart)
rpart(Survived ~ ., train, method = "class")(train)
tree <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data=train,method="class")
#tree <- rpart(Survived ~ ., train, method = "class")
pred <- predict(tree,test,type = "class")
# Calculate the confusion matrix: conf
conf <- table(test$Survived, pred)
# Print this confusion matrix
print(conf)
library(rpart)
rpart(Survived ~ ., train, method = "class")(train)
tree <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data=train,method="class")
#tree <- rpart(Survived ~ ., train, method = "class")
pred <- predict(tree,test,type = "class")
# Calculate the confusion matrix: conf
conf <- table(test$Survived, pred)
# Print this confusion matrix
print(conf)
library(rpart)
rpart(Survived ~ ., train, method = "class")(train)
tree <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data=train,method="class")
#tree <- rpart(Survived ~ ., train, method = "class")
pred <- predict(tree,test,type = "class")
# Print this confusion matrix
print(pred)
plot(tree)
text(fit)
source('C:/MyStuff/Machine Learning/Datacamp/ClassficationProject/DecisionTreeExample.R')
source('C:/MyStuff/Machine Learning/Datacamp/ClassficationProject/DecisionTreeExample.R')
source('C:/MyStuff/Machine Learning/Datacamp/ClassficationProject/DecisionTreeExample.R')
source('C:/MyStuff/Machine Learning/Datacamp/ClassficationProject/DecisionTreeExample.R')
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
install.packages("RColorBrewer")
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
install.packages("rpart.plot")
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
tree <- rpart(Survived ~ ., train, method = "class")
# Draw the decision tree
fancyRpartPlot(tree)
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
tree <- rpart(Survived ~ ., train, method = "class")
# Draw the decision tree
fancyRpartPlot(tree)
install.packages("rattle")
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
tree <- rpart(Survived ~ ., train, method = "class")
# Draw the decision tree
fancyRpartPlot(tree)
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
tree <- rpart(Survived ~ ., train, method = "class")
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
tree <- rpart(Survived ~ ., train, method = "class")
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
tree <- rpart(Survived ~ ., train, method = "class")
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
tree <- rpart(Survived ~ ., train, method = "class")
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
tree <- rpart(Survived ~ ., train, method = "class")
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
tree <- rpart(Survived ~ ., train, method = "class")
-version
r --version
source('C:/MyStuff/Machine Learning/Datacamp/ClassficationProject/DecisionTreeExample.R')
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
str(train)
prop.table(table(train$Survived))
test$Survived <- rep(0, 418)
#1. Let's set everyone got killed in test set
submit <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit, file = "theyallperish.csv", row.names = FALSE)
summary(train$Sex)
prop.table(table(train$Sex, train$Survived))
prop.table(table(train$Sex, train$Survived),1)
#2. Let's set every female survied
test$Survived <- 0
test$Survived[test$Sex == 'female'] <- 1
submit2 <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit2, file = "theyallperish2.csv", row.names = FALSE)
summary(train$Age)
#let’s create a new variable, “Child”, to indicate whether the passenger is below the age of 18
train$Child <- 0
train$Child[train$Age < 18] <- 1
#create a table with both gender and age to see the survival proportions for different subsets
aggregate(Survived ~ Child + Sex, data=train, FUN=sum)
# Now we have the totals for each group of passengers, but really,
#we would like to know the proportions again. To do this is a little more complicated.
#We need to create a function that takes the subset vector as input and applies both the sum and length commands to it,
#and then does the division to give us a proportion.
aggregate(Survived ~ Child + Sex, data=train, FUN=function(x) {sum(x)/length(x)})
#Let’s bin the fares into less than $10, between $10 and $20, $20 to $30 and more than $30 and store it to a new variable:
train$Fare2 <- '30+'
train$Fare2[train$Fare < 30 & train$Fare >= 20] <- '20-30'
train$Fare2[train$Fare < 20 & train$Fare >= 10] <- '10-20'
train$Fare2[train$Fare < 10] <- '<10'
aggregate(Survived ~ Fare2 + Pclass + Sex, data=train, FUN=function(x) {sum(x)/length(x)})
#3.let’s make a new prediction based on the new insights.
test$Survived <- 0
test$Survived[test$Sex == 'female'] <- 1
test$Survived[test$Sex == 'female' & test$Pclass == 3 & test$Fare >= 20] <- 0
submit3 <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit3, file = "theyallperish3.csv", row.names = FALSE)
#4.Let's use machine learning to buid decision tree to do the prediction
fit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data = train, method = "class")
plot(fit)
text(fit)
#use Rattle() to create a nice plotting
fancyRpartPlot(fit)
#make a prediction using test set
Prediction <- predict(fit, test, type = "class")
submit4 <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
write.csv(submit4, file = "myfirstdtree.csv", row.names = FALSE)
#5.Overwrite the default settings of rpart() to increase the accuracy
#The rpart package automatically caps the depth that the tree grows by using a metric called complexity
#which stops the resulting model from getting too out of hand.
#But we already saw that a more complex model than what we made ourselves did a bit better,
#so why not go all out and override the defaults? Let’s do it.
fit2 <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
data=train,
method="class",
control=rpart.control(minsplit=2, cp=0))
fancyRpartPlot(fit2)
source('C:/MyStuff/Machine Learning/Datacamp/ClassficationProject/DecisionTreeExample.R', encoding = 'UTF-8')
source('C:/MyStuff/Machine Learning/Datacamp/ClassficationProject/DecisionTreeExample.R', encoding = 'UTF-8')
write.csv(submit4, file = "myfirstdtree2.csv", row.names = FALSE)
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
str(train)
prop.table(table(train$Survived))
test$Survived <- rep(0, 418)
#1. Let's set everyone got killed in test set
submit <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit, file = "theyallperish.csv", row.names = FALSE)
summary(train$Sex)
prop.table(table(train$Sex, train$Survived))
prop.table(table(train$Sex, train$Survived),1)
#2. Let's set every female survied
test$Survived <- 0
test$Survived[test$Sex == 'female'] <- 1
submit2 <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit2, file = "theyallperish2.csv", row.names = FALSE)
summary(train$Age)
#let’s create a new variable, “Child”, to indicate whether the passenger is below the age of 18
train$Child <- 0
train$Child[train$Age < 18] <- 1
#create a table with both gender and age to see the survival proportions for different subsets
aggregate(Survived ~ Child + Sex, data=train, FUN=sum)
# Now we have the totals for each group of passengers, but really,
#we would like to know the proportions again. To do this is a little more complicated.
#We need to create a function that takes the subset vector as input and applies both the sum and length commands to it,
#and then does the division to give us a proportion.
aggregate(Survived ~ Child + Sex, data=train, FUN=function(x) {sum(x)/length(x)})
#Let’s bin the fares into less than $10, between $10 and $20, $20 to $30 and more than $30 and store it to a new variable:
train$Fare2 <- '30+'
train$Fare2[train$Fare < 30 & train$Fare >= 20] <- '20-30'
train$Fare2[train$Fare < 20 & train$Fare >= 10] <- '10-20'
train$Fare2[train$Fare < 10] <- '<10'
aggregate(Survived ~ Fare2 + Pclass + Sex, data=train, FUN=function(x) {sum(x)/length(x)})
#3.let’s make a new prediction based on the new insights.
test$Survived <- 0
test$Survived[test$Sex == 'female'] <- 1
test$Survived[test$Sex == 'female' & test$Pclass == 3 & test$Fare >= 20] <- 0
submit3 <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit3, file = "theyallperish3.csv", row.names = FALSE)
#4.Let's use machine learning to buid decision tree to do the prediction
fit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data = train, method = "class")
plot(fit)
text(fit)
#use Rattle() to create a nice plotting
fancyRpartPlot(fit)
#make a prediction using test set
Prediction <- predict(fit, test, type = "class")
submit4 <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
write.csv(submit4, file = "myfirstdtree.csv", row.names = FALSE)
#5.Overwrite the default settings of rpart() to increase the accuracy
#The rpart package automatically caps the depth that the tree grows by using a metric called complexity
#which stops the resulting model from getting too out of hand.
#But we already saw that a more complex model than what we made ourselves did a bit better,
#so why not go all out and override the defaults? Let’s do it.
fit2 <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
data=train,
method="class",
control=rpart.control(minsplit=2, cp=0))
fancyRpartPlot(fit2)
Prediction2 <- predict(fit2, test, type = "class")
submit5 <- data.frame(PassengerId = test$PassengerId, Survived = Prediction2)
write.csv(submit4, file = "myfirstdtree2.csv", row.names = FALSE)
#Overfitting is technically defined as a model that performs better on a training set than another simpler model,but does worse on unseen data, as we saw here.
#We went too far and grew our decision tree out to encompass massively complex rules that may not generalize to unknown passengers.
#You can also manually trim trees in R with these commands:
new.fit <- prp(fit,snip=TRUE)$obj
fancyRpartPlot(new.fit)
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
str(train)
prop.table(table(train$Survived))
test$Survived <- rep(0, 418)
#1. Let's set everyone got killed in test set
submit <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit, file = "theyallperish.csv", row.names = FALSE)
summary(train$Sex)
prop.table(table(train$Sex, train$Survived))
prop.table(table(train$Sex, train$Survived),1)
#2. Let's set every female survied
test$Survived <- 0
test$Survived[test$Sex == 'female'] <- 1
submit2 <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit2, file = "theyallperish2.csv", row.names = FALSE)
summary(train$Age)
#let’s create a new variable, “Child”, to indicate whether the passenger is below the age of 18
train$Child <- 0
train$Child[train$Age < 18] <- 1
#create a table with both gender and age to see the survival proportions for different subsets
aggregate(Survived ~ Child + Sex, data=train, FUN=sum)
# Now we have the totals for each group of passengers, but really,
#we would like to know the proportions again. To do this is a little more complicated.
#We need to create a function that takes the subset vector as input and applies both the sum and length commands to it,
#and then does the division to give us a proportion.
aggregate(Survived ~ Child + Sex, data=train, FUN=function(x) {sum(x)/length(x)})
#Let’s bin the fares into less than $10, between $10 and $20, $20 to $30 and more than $30 and store it to a new variable:
train$Fare2 <- '30+'
train$Fare2[train$Fare < 30 & train$Fare >= 20] <- '20-30'
train$Fare2[train$Fare < 20 & train$Fare >= 10] <- '10-20'
train$Fare2[train$Fare < 10] <- '<10'
aggregate(Survived ~ Fare2 + Pclass + Sex, data=train, FUN=function(x) {sum(x)/length(x)})
#3.let’s make a new prediction based on the new insights.
test$Survived <- 0
test$Survived[test$Sex == 'female'] <- 1
test$Survived[test$Sex == 'female' & test$Pclass == 3 & test$Fare >= 20] <- 0
submit3 <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit3, file = "theyallperish3.csv", row.names = FALSE)
#4.Let's use machine learning to buid decision tree to do the prediction
fit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data = train, method = "class")
plot(fit)
text(fit)
#use Rattle() to create a nice plotting
fancyRpartPlot(fit)
#make a prediction using test set
Prediction <- predict(fit, test, type = "class")
submit4 <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
write.csv(submit4, file = "myfirstdtree.csv", row.names = FALSE)
#5.Overwrite the default settings of rpart() to increase the accuracy
#The rpart package automatically caps the depth that the tree grows by using a metric called complexity
#which stops the resulting model from getting too out of hand.
#But we already saw that a more complex model than what we made ourselves did a bit better,
#so why not go all out and override the defaults? Let’s do it.
fit2 <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
data=train,
method="class",
control=rpart.control(minsplit=2, cp=0))
fancyRpartPlot(fit2)
Prediction2 <- predict(fit2, test, type = "class")
submit5 <- data.frame(PassengerId = test$PassengerId, Survived = Prediction2)
write.csv(submit4, file = "myfirstdtree2.csv", row.names = FALSE)
#Overfitting is technically defined as a model that performs better on a training set than another simpler model,but does worse on unseen data, as we saw here.
#We went too far and grew our decision tree out to encompass massively complex rules that may not generalize to unknown passengers.
#You can also manually trim trees in R with these commands:
new.fit <- prp(fit,snip=TRUE)$obj
fancyRpartPlot(new.fit)
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
str(train)
prop.table(table(train$Survived))
test$Survived <- rep(0, 418)
#1. Let's set everyone got killed in test set
submit <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit, file = "theyallperish.csv", row.names = FALSE)
summary(train$Sex)
prop.table(table(train$Sex, train$Survived))
prop.table(table(train$Sex, train$Survived),1)
#2. Let's set every female survied
test$Survived <- 0
test$Survived[test$Sex == 'female'] <- 1
submit2 <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit2, file = "theyallperish2.csv", row.names = FALSE)
summary(train$Age)
#let’s create a new variable, “Child”, to indicate whether the passenger is below the age of 18
train$Child <- 0
train$Child[train$Age < 18] <- 1
#create a table with both gender and age to see the survival proportions for different subsets
aggregate(Survived ~ Child + Sex, data=train, FUN=sum)
# Now we have the totals for each group of passengers, but really,
#we would like to know the proportions again. To do this is a little more complicated.
#We need to create a function that takes the subset vector as input and applies both the sum and length commands to it,
#and then does the division to give us a proportion.
aggregate(Survived ~ Child + Sex, data=train, FUN=function(x) {sum(x)/length(x)})
#Let’s bin the fares into less than $10, between $10 and $20, $20 to $30 and more than $30 and store it to a new variable:
train$Fare2 <- '30+'
train$Fare2[train$Fare < 30 & train$Fare >= 20] <- '20-30'
train$Fare2[train$Fare < 20 & train$Fare >= 10] <- '10-20'
train$Fare2[train$Fare < 10] <- '<10'
aggregate(Survived ~ Fare2 + Pclass + Sex, data=train, FUN=function(x) {sum(x)/length(x)})
#3.let’s make a new prediction based on the new insights.
test$Survived <- 0
test$Survived[test$Sex == 'female'] <- 1
test$Survived[test$Sex == 'female' & test$Pclass == 3 & test$Fare >= 20] <- 0
submit3 <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit3, file = "theyallperish3.csv", row.names = FALSE)
#4.Let's use machine learning to buid decision tree to do the prediction
fit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data = train, method = "class")
plot(fit)
text(fit)
#use Rattle() to create a nice plotting
fancyRpartPlot(fit)
#make a prediction using test set
Prediction <- predict(fit, test, type = "class")
submit4 <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
write.csv(submit4, file = "myfirstdtree.csv", row.names = FALSE)
#5.Overwrite the default settings of rpart() to increase the accuracy
#The rpart package automatically caps the depth that the tree grows by using a metric called complexity
#which stops the resulting model from getting too out of hand.
#But we already saw that a more complex model than what we made ourselves did a bit better,
#so why not go all out and override the defaults? Let’s do it.
fit2 <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,
data=train,
method="class",
control=rpart.control(minsplit=2, cp=0))
fancyRpartPlot(fit2)
Prediction2 <- predict(fit2, test, type = "class")
submit5 <- data.frame(PassengerId = test$PassengerId, Survived = Prediction2)
write.csv(submit4, file = "myfirstdtree2.csv", row.names = FALSE)
#Overfitting is technically defined as a model that performs better on a training set than another simpler model,but does worse on unseen data, as we saw here.
#We went too far and grew our decision tree out to encompass massively complex rules that may not generalize to unknown passengers.
#You can also manually trim trees in R with these commands:
new.fit <- prp(fit,snip=TRUE)$obj
